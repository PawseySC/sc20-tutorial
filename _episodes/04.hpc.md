---
title: "High performance containers"
teaching: 10
exercises: 15
questions:
- 
objectives:
- Discuss the steps required to configure and run MPI applications from a container
- Discuss the performance of parallel applications inside containers *versus* regular runs
- Get started with Nvidia GPU containers for HPC applications
keypoints:
- You need to build your application in the container with an MPI version which is ABI compatible with MPI libraries in the host
- Appropriate environment variables and bind mounts are required at runtime to make the most out of MPI applications (sys admins can help)
- Singularity interfaces almost transparently with HPC schedulers such as Slurm
- MPI performance of containerised applications almost coincide with those of a native run
- You can run containerised GPU applications using the flag `--nv`
- By using this flag, Singularity will look for GPU drivers in the host for you, and bind mount them in the container
---


### Recorded presentation

[AVAILABLE AFTER SC20]

Key concepts around how to use containers on HPC with Singularity and Shifter are presented in this video:

<img src="{{ page.root }}/fig/vid.png" alt="Placeholder for Video Lecture" width="150">

<!--
<a href="{{ page.root }}/videos/vid4.mp4" title="Video Lecture"><img src="{{ page.root }}/fig/vid.png" alt="Link to Video Lecture" width="150"></a>
-->


### Let's run OpenFoam in a container!

We're going to start this episode with actually running a practical example, and then discuss the way this all works later on.  
We're using OpenFoam, a widely popular package for Computational Fluid Dynamics simulations, which is able to massively scale in parallel architectures up to thousands of processes, by leveraging an MPI library.  
The sample inputs come straight from the OpenFoam installation tree, namely `$FOAM_TUTORIALS/incompressible/pimpleFoam/LES/periodicHill/steadyState/`.

First, let us cd into the demo directory and download the OpenFoam container image:

```
$ cd $TUTO/demos/openfoam
$ singularity pull library://marcodelapierre/beta/openfoam:v1812
```
{: .bash}

Now, let us run the sample simulation with:

```
$ ./mpi_mpirun.sh
```
{: .bash}

**In alternative**, if you're running this example on Pawsey systems (*e.g.* Magnus or Zeus), achieve the same result by using the Slurm scheduler to submit the job script `mpi_pawsey.sh`:

```
$ sbatch mpi_pawsey.sh
```
{: .bash}

The run will take a couple of minutes. When it's finished, the directory contents will look a bit like this one:

```
$ ls -ltr
```
{: .bash}

```
total 80
-rwxr-xr-x 1 user000 tutorial  1304 Nov 16 17:36 update-settings.sh
drwxr-xr-x 2 user000 tutorial   141 Nov 16 17:36 system
-rw-r--r-- 1 user000 tutorial   871 Nov 16 17:36 mpi_pawsey.sh
-rwxr-xr-x 1 user000 tutorial   789 Nov 16 17:36 mpi_mpirun.sh
drwxr-xr-x 2 user000 tutorial    59 Nov 16 17:36 0
drwxr-xr-x 4 user000 tutorial    72 Nov 16 22:45 dynamicCode
drwxr-xr-x 3 user000 tutorial    77 Nov 16 22:45 constant
-rw-rw-r-- 1 user000 tutorial  3493 Nov 16 22:45 log.blockMesh
-rw-rw-r-- 1 user000 tutorial  1937 Nov 16 22:45 log.topoSet
-rw-rw-r-- 1 user000 tutorial  2300 Nov 16 22:45 log.decomposePar
drwxr-xr-x 8 user000 tutorial    70 Nov 16 22:47 processor1
drwxr-xr-x 8 user000 tutorial    70 Nov 16 22:47 processor0
-rw-rw-r-- 1 user000 tutorial 18569 Nov 16 22:47 log.simpleFoam
drwxr-xr-x 3 user000 tutorial    76 Nov 16 22:47 20
-rw-r--r-- 1 user000 tutorial 28617 Nov 16 22:47 slurm-10.out
-rw-rw-r-- 1 user000 tutorial  1529 Nov 16 22:47 log.reconstructPar
```
{: .output}

We ran using *2 MPI* processes, who created outputs in the directories `processor0` and `processor1`, respectively.  The final reconstruction creates results in the directory `20` (which stands for the *20th* and last simulation step in this very short demo run).

What has just happened?


### A batch script for MPI applications with containers

Let's have a look at the content of the script `mpi_mpirun.sh`:

```
#!/bin/bash

NTASKS="2"

# this configuration depends on the host
export MPICH_ROOT="/opt/mpich/mpich-3.1.4/apps"

export SINGULARITY_BINDPATH="$MPICH_ROOT"
export SINGULARITYENV_LD_LIBRARY_PATH="$MPICH_ROOT/lib"


# pre-processing
singularity exec openfoam_v1812.sif \
  blockMesh | tee log.blockMesh

singularity exec openfoam_v1812.sif \
  topoSet | tee log.topoSet

singularity exec openfoam_v1812.sif \
  decomposePar -fileHandler uncollated | tee log.decomposePar


# run OpenFoam with MPI
mpirun -n $NTASKS \
  singularity exec openfoam_v1812.sif \
  simpleFoam -fileHandler uncollated -parallel | tee log.simpleFoam


# post-processing
singularity exec openfoam_v1812.sif \
  reconstructPar -latestTime -fileHandler uncollated | tee log.reconstructPar
```
{: .bash}


### How does Singularity interplay with the MPI launcher?

We'll comment on the environment variable definitions soon, now let's focus on the set of commands that make the simulation happen.

In particular, the fourth command is the only one using multiple processors through MPI:

```
mpirun -n $NTASKS \
  singularity exec openfoam_v1812.sif \
  simpleFoam -fileHandler uncollated -parallel | tee log.simpleFoam
```
{: .bash}

Here, `mpirun` is the MPI launcher, *i.e.* the tool that is in charge for spawning the multiple MPI processes that will make the workflow run in parallel.  
Note how `singularity` can be executed through the launcher as any other application would.

Under the hood, the MPI processes outside of the container (spawned by `mpirun`) will work in tandem with the containerized MPI code to instantiate the job.  
There are a few implications here...


### Bla bla bla

In the current example we have:

```
export MPICH_ROOT="/opt/mpich/mpich-3.1.4/apps"

export SINGULARITY_BINDPATH="$MPICH_ROOT"
export SINGULARITYENV_LD_LIBRARY_PATH="$MPICH_ROOT/lib"
```
{: .bash}

Here, `SINGULARITY_BINDPATH` bind mounts the host path where the MPI installation is (MPICH in this case).  
The second variable, SINGULARITYENV_LD_LIBRARY_PATH, ensures that at runtime the container's `LD_LIBRARY_PATH` has the path to the MPICH libraries.


### Singularity interface to Slurm

Now, if we have a look at the script variant for the Slurm scheduler, `mpi_pawsey.sh`, we'll see the key difference is that every OpenFoam command is executed via `srun`:

```
srun -n $SLURM_NTASKS \
  singularity exec openfoam_v1812.sif \
  simpleFoam -fileHandler uncollated -parallel | tee log.simpleFoam
```
{: .bash}

`srun` is the Slurm wrapper for the MPI launcher, `mpirun`.  Other schedulers will require a different command.  
In practice, all we had to do was to replace `mpirun` with `srun`.  This is because Singularity implements a native interface to schedulers, so it can be executed through `srun` as other packages would.

Note in the script how, when using schedulers, it is good practice to execute all application commands through `srun`, even those that only use one core.  




> ## Note
>
> To run exercises from this episode on your own, you'll need a machine with a GPU card and GPU drivers installed.  
> There are examples for both using and not using the Slurm scheduler.
{: .callout}


### Run a molecular dynamics simulation on a GPU with containers

For our example we are going to use Gromacs, a quite popular molecular dynamics package, among the ones that have been optimised to run on GPUs through Nvidia containers.

First, let us cd into `demos/gromacs` and download the container image `nvcr.io/hpc/gromacs:2018.2`:

```
$ cd $TUTO/demos/gromacs
$ singularity pull docker://nvcr.io/hpc/gromacs:2018.2
```
{: .bash}

The current directory has got sample input files picked from the collection of [Gromacs benchmark examples](ftp://ftp.gromacs.org/pub/benchmarks/water_GMX50_bare.tar.gz).  In particular, we're going to use the subset `water-cut1.0_GMX50_bare/1536/`. First let's `gunzip` one of the required input files:

```
$ gunzip conf.gro.gz
```
{: .bash}

Now, from a Singularity perspective, all we need to do to run a GPU application from a container is to add the runtime flag `--nv`.  This will make Singularity look for the Nvidia drivers in the host, and mount them inside the container.

On the host system side, when running GPU applications through Singularity the only requirement consists of the Nvidia driver for the relevant GPU card (the corresponding file is typically called `libcuda.so.<VERSION>` and is located in some library subdirectory of `/usr`).

Do not execute the next two commands, let us just have a look at them.

* Preliminary step
  ```
  $ singularity exec --nv gromacs_2018.2.sif gmx grompp -f pme.mdp
  ```
  {: .bash}

* Production step
  ```
  $ singularity exec --nv gromacs_2018.2.sif gmx mdrun -ntmpi 1 -nb gpu -pin on -v -noconfout -nsteps 5000 -s topol.tpr -ntomp 1
  ```
  {: .bash}

GPU resources are usually made available in HPC systems through schedulers, to which Singularity natively and transparently interfaces.  So, for instance let us have a look in the current directory at the Slurm batch script called `gpu.sh`, which is suitable for running at Pawsey on *Zeus* or *Topaz*:

```
#!/bin/bash -l

#SBATCH --job-name=gpu
#SBATCH --partition=gpuq
#SBATCH --gres=gpu:1
#SBATCH --ntasks=1
#SBATCH --time=01:00:00

module load singularity

# run Gromacs preliminary step with container
srun singularity exec --nv gromacs_2018.2.sif \
    gmx grompp -f pme.mdp

# Run Gromacs MD with container
srun singularity exec --nv gromacs_2018.2.sif \
    gmx mdrun -ntmpi 1 -nb gpu -pin on -v -noconfout -nsteps 5000 -s topol.tpr -ntomp 1
```
{: .bash}

Basically, we have just combined the Slurm command `srun` with `singularity exec <..>` (similar to what we did in the episode on MPI).  We can submit the script with:

```
$ sbatch gpu.sh
```
{: .bash}
